{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "###In which we explore Segmenting Frequent InterGallacticHoppers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "* ###InterGallactic Airlines have the GallacticHoppers frequent flyer program & have data about their customers who participate in the program. \n",
    "* ###The airlines execs have a feeling that other airlines will poach their customers if they do not keep their loyal customers happy.\n",
    "* ###So the business want to customize promotions to their frequent flier program.\n",
    "* ###Can they just have one type of promotion ? \n",
    "* ###Should they have different types of incentives ?\n",
    "* ###Who exactly are the customers in their GallacticHoppers program ?\n",
    "* ###Recently they have deployed an infrastructure with Spark\n",
    "* ###Can Spark help in this business problem ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "print \"Last run @%s\" % (datetime.datetime.now(timezone('US/Pacific')))\n",
    "#\n",
    "from pyspark.context import SparkContext\n",
    "print \"Running Spark Version %s\" % (sc.version)\n",
    "#\n",
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print conf.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "freq_df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "            .options(header='true')\\\n",
    "            .load('freq-flyer/AirlinesCluster.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### But we need a table of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "freq_rdd = freq_df.map(lambda row: array([float(x) for x in row]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_rdd.first()\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(KMeans.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km_mdl_1 = KMeans.train(freq_rdd, 2, maxIterations=10,runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in km_mdl_1.clusterCenters:\n",
    "        print \"%10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (x[0],x[1],x[2],x[3],x[4],x[5],x[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in freq_rdd.take(10):\n",
    "    print x,km_mdl_1.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def squared_error(mdl, point):\n",
    "    center = mdl.centers[mdl.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WSSSE = freq_rdd.map(lambda point: squared_error(km_mdl_1,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(freq_rdd)\n",
    "print summary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (summary.mean()[0],summary.mean()[1],summary.mean()[2],\n",
    "                                                            summary.mean()[3],summary.mean()[4],summary.mean()[5],\n",
    "                                                            summary.mean()[6])\n",
    "print \"Max  : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (summary.max()[0],summary.max()[1],\n",
    "                                                                       summary.max()[2],\n",
    "                                                            summary.max()[3],summary.max()[4],summary.max()[5],\n",
    "                                                            summary.max()[6])\n",
    "print \"Min  : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (summary.min()[0],summary.min()[1],\n",
    "                                                                       summary.min()[2],\n",
    "                                                            summary.min()[3],summary.min()[4],summary.min()[5],\n",
    "                                                            summary.min()[6])\n",
    "print \"Variance : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (summary.variance()[0],summary.variance()[1],\n",
    "                                                                       summary.variance()[2],\n",
    "                                                            summary.variance()[3],summary.variance()[4],summary.variance()[5],\n",
    "                                                            summary.variance()[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You see, K-means clustering is \"isotropic\" in all directions of space and therefore tends to produce \n",
    "# more or less round (rather than elongated) clusters. [Ref 2]\n",
    "# In this situation leaving variances unequal is equivalent to putting more weight on variables with smaller variance, \n",
    "# so clusters will tend to be separated along variables with greater variance. [Ref 3]\n",
    "#\n",
    "# center, scale, box-cox, preprocess in caret\n",
    "# zero mean and unit variance\n",
    "#\n",
    "# (x - mu)/sigma\n",
    "# org.apache.spark.mlib.feature.StandardScaler does this, but to the best of my knowledge \n",
    "#            as of now (9/28/14) not available for python \n",
    "# So we do it manually, gives us a chance to do some functional programming !\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_mean = summary.mean()\n",
    "data_sigma = summary.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in data_sigma:\n",
    "    print x,sqrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def center_and_scale(a_record):\n",
    "    for i in range(len(a_record)):\n",
    "        a_record[i] = (a_record[i] - data_mean[i])/sqrt(data_sigma[i]) # (x-mean)/sd\n",
    "    return a_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_norm_rdd = freq_rdd.map(lambda x: center_and_scale(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_norm_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let us try with the standardized data\n",
    "km_mdl_std = KMeans.train(freq_norm_rdd, 2, maxIterations=10,runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in km_mdl_std.clusterCenters:\n",
    "        print \"%10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (x[0],x[1],x[2],x[3],x[4],x[5],x[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WSSSE = freq_norm_rdd.map(lambda point: squared_error(km_mdl_std,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let us try with k= 5 clusters instead of k=2\n",
    "km_mdl_std_5 = KMeans.train(freq_norm_rdd, 5, maxIterations=10,runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in km_mdl_std_5.clusterCenters:\n",
    "        print \"%10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (x[0],x[1],x[2],x[3],x[4],x[5],x[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WSSSE = freq_norm_rdd.map(lambda point: squared_error(km_mdl_std_5,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km_mdl_std_10 = KMeans.train(freq_norm_rdd, 10, maxIterations=10,runs=10, initializationMode=\"random\")\n",
    "for x in km_mdl_std_10.clusterCenters:\n",
    "        print \"%10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (x[0],x[1],x[2],x[3],x[4],x[5],x[6])\n",
    "#\n",
    "WSSSE = freq_norm_rdd.map(lambda point: squared_error(km_mdl_std_10,point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_rdd = freq_norm_rdd.map(lambda x: km_mdl_std_5.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_rdd_1 = inp_file.map(lambda line: array([int(x) for x in line.split(',')]))\n",
    "freq_cluster_map = freq_rdd_1.zip(cluster_rdd)\n",
    "freq_cluster_map.take(5) \n",
    "# Gives org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_cluster_map = inp_file.map(lambda line: array([int(x) for x in line.split(',')])).zip(cluster_rdd)\n",
    "freq_cluster_map.take(5) \n",
    "# Gives org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_cluster_map = freq_rdd.zip(cluster_rdd)\n",
    "freq_cluster_map.take(5) # This works !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_0 = freq_cluster_map.filter(lambda x: x[1] == 0)\n",
    "cluster_1 = freq_cluster_map.filter(lambda x: x[1] == 1)\n",
    "cluster_2 = freq_cluster_map.filter(lambda x: x[1] == 2)\n",
    "cluster_3 = freq_cluster_map.filter(lambda x: x[1] == 3)\n",
    "cluster_4 = freq_cluster_map.filter(lambda x: x[1] == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cluster_0.count()\n",
    "print cluster_1.count()\n",
    "print cluster_2.count()\n",
    "print cluster_3.count()\n",
    "print cluster_4.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "143\n",
    "1372\n",
    "768\n",
    "59\n",
    "1657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_0.count()+cluster_1.count()+cluster_2.count()+cluster_3.count()+cluster_4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_rdd_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_cluster_map.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_0.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stat_0 = Statistics.colStats(cluster_0.map(lambda x: x[0]))\n",
    "stat_1 = Statistics.colStats(cluster_1.map(lambda x: x[0]))\n",
    "stat_2 = Statistics.colStats(cluster_2.map(lambda x: x[0]))\n",
    "stat_3 = Statistics.colStats(cluster_3.map(lambda x: x[0]))\n",
    "stat_4 = Statistics.colStats(cluster_4.map(lambda x: x[0]))\n",
    "print \"0 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_0.mean()[0],stat_0.mean()[1],stat_0.mean()[2],\n",
    "                                                            stat_0.mean()[3],stat_0.mean()[4],stat_0.mean()[5],\n",
    "                                                            stat_0.mean()[6])\n",
    "print \"1 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_1.mean()[0],stat_1.mean()[1],stat_1.mean()[2],\n",
    "                                                            stat_1.mean()[3],stat_1.mean()[4],stat_1.mean()[5],\n",
    "                                                            stat_1.mean()[6])\n",
    "print \"2 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_2.mean()[0],stat_2.mean()[1],stat_2.mean()[2],\n",
    "                                                            stat_2.mean()[3],stat_2.mean()[4],stat_2.mean()[5],\n",
    "                                                            stat_2.mean()[6])\n",
    "print \"3 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_3.mean()[0],stat_3.mean()[1],stat_3.mean()[2],\n",
    "                                                            stat_3.mean()[3],stat_3.mean()[4],stat_3.mean()[5],\n",
    "                                                            stat_3.mean()[6])\n",
    "print \"4 : %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f %10.3f\" % (stat_4.mean()[0],stat_4.mean()[1],stat_4.mean()[2],\n",
    "                                                            stat_4.mean()[3],stat_4.mean()[4],stat_4.mean()[5],\n",
    "                                                            stat_4.mean()[6])\n",
    "# Balance, TopStatusQualMiles, NonFlightMiles, NonFlightTrans, FlightMiles, FlightTrans, DaysSinceEnroll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "One run on Sep 27:\n",
    "0 :  37950.925     33.352   6660.214      7.644    183.511      0.567   2220.540 # Relatively new, not active\n",
    "1 :  56183.841     54.051   8370.021      8.902    205.035      0.620   5748.698\n",
    "2 : 117326.186   5445.305  19059.610     12.305    965.797      2.881   3874.831 # Top Status Qual Miles\n",
    "3 : 191736.336    471.566  33093.336     28.357   5763.133     16.769   4666.413 # Most Active\n",
    "4 : 150843.700     73.158  50474.264     21.183    473.292      1.441   4938.489 # non-flight but active customers\n",
    "````\n",
    "````\n",
    "Run 10/28/14\n",
    "0 :  38091.905     32.784   6731.402      7.630    178.718      0.555   2281.777\n",
    "1 :  57441.909     55.024   8758.131      9.104    213.633      0.646   5823.841\n",
    "2 : 191736.336    471.566  33093.336     28.357   5763.133     16.769   4666.413\n",
    "3 : 117326.186   5445.305  19059.610     12.305    965.797      2.881   3874.831\n",
    "4 : 152607.968     74.778  51066.228     21.329    478.139      1.449   4913.985\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Different runs will produce different clusters\n",
    "# Once the model is executed, the characteristics can interpreted & used in business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
