{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Titanic Dataset\n",
    "### In which we explore Disasters, Trees, Classification & the Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visit Kaggle and download Data from http://www.kaggle.com/c/titanic-gettingStarted\n",
    "2. Read Titanic Data\n",
    "2. Transform and select features\n",
    "3. Create a simple model & Predict\n",
    "4. Submit to Kaggle & checkout the leaderboard\n",
    "5. Decision Tree Model, Predict & Submit\n",
    "6. Random Forest Model, Predict & Submit\n",
    "7. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pytz import timezone\n",
    "print \"Last run @%s\" % (datetime.datetime.now(timezone('US/Pacific')))\n",
    "#\n",
    "from pyspark.context import SparkContext\n",
    "print \"Running Spark Version %s\" % (sc.version)\n",
    "#\n",
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print conf.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Read Titanic Data\n",
    "### The Data is part of the Kaggle Competition \"Titanic: Machine Learning from Disaster\"\n",
    "### Download data from http://www.kaggle.com/c/titanic-gettingStarted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read Train & Test Datasets\n",
    "train = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('titanic-r/train.csv')\n",
    "test = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('titanic-r/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "train_1 = train.select(train['PassengerId'], \n",
    "                 train['Survived'].cast(\"integer\").alias(\"Survived\"),\n",
    "                 train['Pclass'].cast(\"integer\").alias(\"Pclass\"),\n",
    "                 F.when(train['Sex'] == 'female', 1).otherwise(0).alias(\"Gender\"), \n",
    "                 train['Age'].cast(\"integer\").alias(\"Age\"),\n",
    "                 train['SibSp'].cast(\"integer\").alias(\"SibSp\"),\n",
    "                 train['Parch'].cast(\"integer\").alias(\"Parch\"),\n",
    "                 train['Fare'].cast(\"float\").alias(\"Fare\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_1.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Replace null age by 30\n",
    "# Do we have nulls ?\n",
    "train_1.filter(train_1['Age'].isNull()).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Replace null age by 30\n",
    "train_1.na.fill(30,'Age').show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace null age by 30\n",
    "train_2 = train_1.na.fill(30,'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_2.crosstab(\"Gender\",\"Survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"F = %3.2f%% M = %3.2f%%\" % ( (100*233.0/(233+81)), (100*109.0/(109+468)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dick, The butcher to Jack Cade\n",
    "### Dick: The first thing we do, let's kill all the men.\n",
    "### Cade: Nay, that I mean to do.\n",
    "#### Ref : http://www.enotes.com/shakespeare-quotes/lets-kill-all-lawyers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# 1 : Simple Model (M=Survived) \n",
    "#\n",
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = test.select(test['PassengerId'], \n",
    "                 F.when(test['Sex'] == 'female', 1).otherwise(0).alias(\"Survived\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv')\\\n",
    ".options(header='true').save('titanic-r/spark-sub-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "# Rank : 2586 Score : 0.76555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Would age be a better predictor ?\n",
    "#\n",
    "train_1.na.drop().crosstab(\"Age\",\"Survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# *** Home work : See if Pclass, SibSp or Parch is a better indication and change survival accordinglyÂ¶\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def parse_passenger_list(r):\n",
    "    return LabeledPoint(r[1],[r[2],r[3],r[4],r[5],r[6],r[7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rdd = train_2.map(lambda x: parse_passenger_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "model = DecisionTree.trainClassifier(train_rdd, numClasses=2,categoricalFeaturesInfo={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "# print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform test and predict\n",
    "import pyspark.sql.functions as F\n",
    "test_1 = test.select(test['PassengerId'], \n",
    "                 test['Pclass'].cast(\"integer\").alias(\"Pclass\"),\n",
    "                 F.when(test['Sex'] == 'female', 1).otherwise(0).alias(\"Gender\"), \n",
    "                 test['Age'].cast(\"integer\").alias(\"Age\"),\n",
    "                 test['SibSp'].cast(\"integer\").alias(\"SibSp\"),\n",
    "                 test['Parch'].cast(\"integer\").alias(\"Parch\"),\n",
    "                 test['Fare'].cast(\"float\").alias(\"Fare\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do we have nulls ?\n",
    "test_1.filter(test_1['Age'].isNull()).show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_1.groupBy().avg('Age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace null age by 30.24 - the mean\n",
    "test_2 = test_1.na.fill(30,'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse test data for predictions\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def parse_test(r):\n",
    "    return (r[1],r[2],r[3],r[4],r[5],r[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_rdd = test_2.map(lambda x: parse_test(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_rdd = test_2.map(lambda x: x[0]).zip(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_df = out_rdd.toDF(['PassengerId','Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_1 = out_df.select(out_df['PassengerId'],\n",
    "                      out_df['Survived'].cast('integer').alias('Survived'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_1.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_1.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv')\\\n",
    ".options(header='true').save('titanic-r/spark-sub-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "# Rank : 2038 +549 Score : 0.77512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "model_rf = RandomForest.trainClassifier(train_rdd, numClasses=2,categoricalFeaturesInfo={},numTrees=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(model_rf)\n",
    "#print(model_rf.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_rf = model_rf.predict(test_rdd).coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_rf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_rf = test_2.map(lambda x: x[0]).coalesce(1).zip(pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_rf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_df_rf = out_rf.toDF(['PassengerId','Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_2 = out_df_rf.select(out_df_rf['PassengerId'],\n",
    "                      out_df_rf['Survived'].cast('integer').alias('Survived'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_2.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv')\\\n",
    ".options(header='true').save('titanic-r/spark-sub-03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submit\n",
    "# Rank : 1550 +488 Score : 0.78469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looks like we are on a roll ! Let us try SVM !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "model_svm = SVMWithSGD.train(train_rdd, iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_svm = model_svm.predict(test_rdd).coalesce(1)\n",
    "out_svm = test_2.map(lambda x: x[0]).coalesce(1).zip(pred_svm)\n",
    "out_df_svm = out_svm.toDF(['PassengerId','Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_3 = out_df_svm.select(out_df_svm['PassengerId'],\n",
    "                      out_df_svm['Survived'].cast('integer').alias('Survived'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_3.coalesce(1).write.mode('overwrite').format('com.databricks.spark.csv')\\\n",
    ".options(header='true').save('titanic-r/spark-sub-04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not good. Only 0.39713 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did Random Forest or SVM do Better ? \n",
    "#### Why ? Why Not ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Science Folk Wisdom\n",
    "http://www.slideshare.net/ksankar/data-science-folk-knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
